import os
import re
import json
import time
from typing import Optional, Type
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from playwright.sync_api import sync_playwright

# ========== ğŸ§© åŸºç¤å·¥å…· ==========
def sanitize_filename(name: str) -> str:
    """ç§»é™¤ä¸åˆæ³•å­—å…ƒ"""
    return re.sub(r'[\\/*?:"<>|]', "", name).strip()

def save_reviews(place_name, place_id, data, base_dir="output"):
    """å„²å­˜è©•è«– JSON"""
    os.makedirs(base_dir, exist_ok=True)
    safe_name = sanitize_filename(place_name)
    path = os.path.join(base_dir, f"reviews_{safe_name[:40]}_{place_id[:6]}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

# ========== ğŸ§  æ ¸å¿ƒçˆ¬èŸ²ï¼ˆèåˆå¯å‹•ç‰ˆï¼‰ ==========
def scrape_reviews_tw(place_id: str, max_reviews: int = 100, duration_limit: int = 20, headless: bool = True):
    """
    Google Maps è©•è«–çˆ¬èŸ²ï¼ˆCL3 ç‰ˆï¼‰
    âœ… å¯è¢« LangChain Agent å‘¼å«
    âœ… è‡ªå‹•æ»¾å‹•ï¼‹åŠ é€Ÿå°é–åœ–ç‰‡
    âœ… æ™‚é–“èˆ‡æ•¸é‡é›™é™åˆ¶
    """
    url = f"https://www.google.com/maps/place/?q=place_id:{place_id}"
    print(f"ğŸŒ é–‹å•Ÿåœ°åœ–é é¢ï¼š{url}")
    print(f"ğŸ“Š æœ€å¤§è©•è«–æ•¸ï¼š{max_reviews}ï¼ˆé™åˆ¶æ™‚é–“ï¼š{duration_limit} ç§’ï¼‰")

    reviews, seen = [], set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        page = browser.new_page()

        # åŠ é€Ÿï¼šå°é–åœ–ç‰‡ã€å½±ç‰‡
        page.route(
            "**/*",
            lambda route: route.abort()
            if route.request.resource_type in ["image", "media"]
            else route.continue_(),
        )

        page.goto(url, timeout=60000)
        page.wait_for_timeout(3000)

        # é»æ“Šã€ŒæŸ¥çœ‹å…¨éƒ¨è©•è«–ã€
        try:
            btn = page.locator("button[aria-label*='è©•è«–'], button[aria-label*='review']").first
            btn.click()
            print("âœ… å·²é»æ“Šè©•è«–æŒ‰éˆ•ï¼Œç­‰å¾…è©•è«–å€é–‹å•Ÿ...")
            page.wait_for_timeout(3000)
            page.wait_for_selector("div[data-review-id]", timeout=15000)
        except Exception as e:
            print(f"âš ï¸ æ‰¾ä¸åˆ°è©•è«–æŒ‰éˆ•æˆ–è¶…æ™‚: {e}")
            browser.close()
            return []

        # æ»¾å‹•è©•è«–
        scroll_script = """
        () => {
            const el = document.querySelector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde');
            if (!el) return 0;
            el.scrollTo(0, el.scrollHeight);
            return el.scrollTop;
        }
        """

        print(f"âš¡ é€£çºŒæ»¾å‹•è©•è«–ä¸­ï¼ˆæœ€é•· {duration_limit} ç§’ï¼‰...")
        start = time.time()
        last_count, no_new = 0, 0

        while len(reviews) < max_reviews and (time.time() - start < duration_limit):
            for _ in range(3):
                page.evaluate(scroll_script)
                page.wait_for_timeout(200)
            page.wait_for_timeout(500)
            count = page.locator("div[data-review-id]").count()
            if count == last_count:
                no_new += 1
            else:
                no_new = 0
            if no_new >= 3:
                print("â¹ ç„¡æ–°è©•è«–ï¼Œåœæ­¢æ»¾å‹•ã€‚")
                break
            last_count = count

            # print(f"ğŸŒ€ å·²è¼‰å…¥ç´„ {count} å‰‡è©•è«–")

        # print(f"âœ… æ»¾å‹•çµæŸï¼Œå…±è¼‰å…¥ {last_count} å‰‡è©•è«–ï¼Œé–‹å§‹è§£æ...")

        elements = page.locator("div[data-review-id]")
        for i in range(elements.count()):
            try:
                el = elements.nth(i)
                text = el.locator("span.wiI7pd, span[jsname='bN97Pc']").first.inner_text(timeout=500)
                stars = el.locator("span[aria-label*='æ˜Ÿ']").first.get_attribute("aria-label")
                match = re.search(r'(\d(?:\.\d)?)', stars or "")
                val = float(match.group(1)) if match else None
                if text.strip() and text.strip() not in seen:
                    seen.add(text.strip())
                    reviews.append({"text": text.strip(), "stars": val})
                    if len(reviews) >= max_reviews:
                        break
            except:
                continue

        print(f"ğŸ¯ æŠ“å–å®Œæˆï¼Œå…± {len(reviews)} å‰‡è©•è«–")
        browser.close()
        return reviews


# ========== âš™ï¸ LangChain Tool ==========
class ReviewScraperInput(BaseModel):
    place_name: str = Field(..., description="åº—å®¶åç¨±")
    place_id: str = Field(..., description="Google Maps Place ID")
    max_reviews: Optional[int] = Field(100, description="æœ€å¤§è©•è«–æ•¸")
    base_dir: Optional[str] = Field("output", description="è¼¸å‡ºè³‡æ–™å¤¾")


class ReviewScraperTool(BaseTool):
    name: str = "review_scraper_tool"
    description: str = "ç”¨æ–¼çˆ¬å– Google Maps çš„è©•è«–è³‡æ–™ï¼ˆç¹é«”ä¸­æ–‡ï¼Œè‡ªå‹•æ»¾å‹•ï¼‰"
    args_schema: Type[BaseModel] = ReviewScraperInput

    def _run(self, place_name: str, place_id: str, max_reviews: int = 100, base_dir: str = "output"):
        data = scrape_reviews_tw(place_id, max_reviews=max_reviews)
        path = save_reviews(place_name, place_id, data, base_dir)
        return {"status": "success", "count": len(data), "file_path": path}

    async def _arun(self, **kwargs):
        raise NotImplementedError("ä¸æ”¯æ´ async æ¨¡å¼")


# ========== ğŸ” å¤–éƒ¨å‡½å¼ï¼ˆä¾› Agent ä½¿ç”¨ï¼‰ ==========
def get_all_reviews(place_name: str, place_id: str, max_reviews: int = 100):
    """å¤–éƒ¨å‘¼å«å°è£ï¼ˆçµ¦ RecommendAgent ç”¨ï¼‰"""
    try:
        data = scrape_reviews_tw(place_id, max_reviews=max_reviews)
        if data:
            save_reviews(place_name, place_id, data, base_dir="data/reviews")
        return data
    except Exception as e:
        print(f"âš ï¸ æŠ“å– {place_name} å¤±æ•—ï¼š{e}")
        return []


# ========== ğŸ§ª æ¸¬è©¦åŸ·è¡Œ ==========
if __name__ == "__main__":
    test_name = "æ‰‹å·¥æ®¿éº»è¾£é‹ç‰© ä¿¡ç¾©åº—"
    test_place_id = "ChIJ-8qspuojaDQRc01XrVuo2sc"

    print("ğŸš€ é–‹å§‹æ¸¬è©¦è©•è«–æ“·å–å·¥å…·ï¼ˆç›®æ¨™ 50 å‰‡ï¼‰...")
    start = time.time()
    reviews = get_all_reviews(test_name, test_place_id, max_reviews=50)

    print(f"\nğŸ“Š å…±æ“·å– {len(reviews)} å‰‡è©•è«–ï¼Œè€—æ™‚ {time.time() - start:.1f} ç§’ã€‚")
    for i, r in enumerate(reviews[:3], 1):
        print(f"{i}. â­{r['stars']}ï¼š{r['text'][:50]}...")
    print("\nğŸ’¾ å·²è‡ªå‹•å„²å­˜ JSONã€‚")
