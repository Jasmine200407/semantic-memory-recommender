# -*- coding: utf-8 -*-
import os
import re
import json
import time
from typing import Optional, Type, List, Dict, Any
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from playwright.sync_api import sync_playwright


def sanitize_filename(name: str) -> str:
    """ç§»é™¤ä¸åˆæ³•å­—å…ƒï¼ˆè‹¥æœªä¾†è¦ç”¨ï¼‰"""
    return re.sub(r'[\\/*?:"<>|]', "", name).strip()


# ==================== ğŸ§  æ ¸å¿ƒçˆ¬èŸ² ==================== #
def scrape_reviews_tw(place_id: str, max_reviews: int = 100, duration_limit: int = 20, headless: bool = True):
    url = f"https://www.google.com/maps/place/?q=place_id:{place_id}"
    print(f"ğŸŒ é–‹å•Ÿåœ°åœ–é é¢ï¼š{url}")
    print(f"ğŸ“Š æœ€å¤§è©•è«–æ•¸ï¼š{max_reviews}ï¼ˆé™åˆ¶æ™‚é–“ï¼š{duration_limit} ç§’ï¼‰")

    reviews, seen = [], set()

    with sync_playwright() as p:
        # ================== â­ Headless Anti-detection ==================
        browser = p.chromium.launch(
            headless=headless,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars",
                "--window-size=1280,800",
                "--no-sandbox",
                "--disable-dev-shm-usage",
            ]
        )

        context = browser.new_context(
            locale="zh-TW",
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1280, "height": 800},
            java_script_enabled=True,
        )

        # â­ æœ€é‡è¦ï¼šè®“ Google ç„¡æ³•åµæ¸¬ headless
        context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        """)

        page = context.new_page()

        # â­ é˜»æ“‹åœ–ç‰‡ï¼ˆä½ åŸæœ¬çš„åŠŸèƒ½ä¿ç•™ï¼‰
        page.route(
            "**/*",
            lambda route: route.abort()
            if route.request.resource_type in ["image", "media"]
            else route.continue_(),
        )

        # ================== â­ é–‹å§‹æµç¨‹ ==================
        page.goto(url, timeout=60000)
        page.wait_for_timeout(2000)

        # é»æ“Šã€ŒæŸ¥çœ‹å…¨éƒ¨è©•è«–ã€æŒ‰éˆ•
        try:
            btn = page.locator("button[aria-label*='è©•è«–'], button[aria-label*='review']").first
            btn.click()
            print("âœ… å·²é»æ“Šè©•è«–æŒ‰éˆ•")
            page.wait_for_timeout(2000)
            page.wait_for_selector("div[data-review-id]", timeout=15000)
        except Exception as e:
            print(f"âš ï¸ æ‰¾ä¸åˆ°è©•è«–æŒ‰éˆ•æˆ–è¶…æ™‚: {e}")
            context.close()
            browser.close()
            return []

        print(f"âš¡ æ­£åœ¨æ»¾å‹•è©•è«–ï¼ˆæœ€é•· {duration_limit} ç§’ï¼‰...")

        scroll_script = """
        () => {
            const el = document.querySelector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde');
            if (!el) return 0;
            el.scrollTo(0, el.scrollHeight);
            return el.scrollTop;
        }
        """

        start_time = time.time()
        while len(reviews) < max_reviews and (time.time() - start_time < duration_limit):
            page.evaluate(scroll_script)
            page.wait_for_timeout(500)

        # â­ æŠ“å–è©•è«–
        elements = page.locator("div[data-review-id]")
        print("ğŸ” æ­£åœ¨è§£æè©•è«–...")

        for i in range(elements.count()):
            try:
                el = elements.nth(i)
                text = el.locator("span.wiI7pd, span[jsname='bN97Pc']").first.inner_text(timeout=500)

                stars_raw = el.locator("span[aria-label*='æ˜Ÿ']").first.get_attribute("aria-label")
                match = re.search(r'(\d(\.\d)?)', stars_raw or "")
                stars = float(match.group(1)) if match else None

                if text not in seen:
                    seen.add(text)
                    reviews.append({"text": text.strip(), "stars": stars})

                    if len(reviews) >= max_reviews:
                        break
            except:
                continue

        print(f"ğŸ¯ å®Œæˆï¼šå…± {len(reviews)} å‰‡è©•è«–")
        context.close()
        browser.close()
        return reviews

# ==================== ğŸ”§ Tool ==================== #
class ReviewScraperInput(BaseModel):
    place_id: str = Field(..., description="Google Maps Place ID")
    max_reviews: Optional[int] = Field(100, description="æœ€å¤§è©•è«–æ•¸")


class ReviewScraperTool(BaseTool):
    name: str = "review_scraper_tool"
    description: str = "çˆ¬å– Google Maps çš„è©•è«–è³‡æ–™ï¼ˆç¹ä¸­ï¼‰"
    args_schema: Type[BaseModel] = ReviewScraperInput

    def _run(self, place_id: str, max_reviews: int = 100):
        # â­ ä¸å­˜æª”ï¼åªé€å‡º reviews
        data = scrape_reviews_tw(place_id, max_reviews=max_reviews)
        return data

    async def _arun(self, **kwargs):
        raise NotImplementedError("ä¸æ”¯æ´ async")


# ==================== ğŸ” æä¾›çµ¦ Agent å‘¼å« ==================== #
def get_all_reviews(place_name: str, place_id: str, max_reviews: int = 100) -> List[Dict[str, Any]]:
    try:
        return scrape_reviews_tw(place_id, max_reviews=max_reviews)
    except Exception as e:
        print(f"âš ï¸ æŠ“å– {place_name} å¤±æ•—ï¼š{e}")
        return []
